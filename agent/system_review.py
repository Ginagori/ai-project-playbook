"""
System Review Module

Meta-analysis of plan vs execution for a project.
Generates a comprehensive review of process quality, artifact quality,
and execution confidence — analyzing the SYSTEM, not just the code.

Inspired by the /system-review pattern from the Agentic Coding Course.
"""

from __future__ import annotations

from agent.models.project import ProjectState, Phase
from agent.evals import ArtifactEvaluator


def generate_system_review(project: ProjectState) -> str:
    """
    Generate a comprehensive system review for a project.

    Analyzes:
    - Plan fidelity (planned vs actual)
    - Artifact quality (CLAUDE.md, PRD scores)
    - Phase progression
    - Feature completion
    - Confidence scoring (1-10)
    - Improvement recommendations

    Args:
        project: The project state to review

    Returns:
        Formatted system review report
    """
    # Calculate metrics
    confidence = calculate_execution_confidence(project)
    phase_analysis = _analyze_phase_progression(project)
    feature_analysis = _analyze_features(project)
    artifact_analysis = _analyze_artifacts(project)
    recommendations = _generate_recommendations(project, confidence)

    report = f"""# System Review: {project.objective}

## Executive Summary

| Metric | Value |
|--------|-------|
| **Confidence Score** | {confidence['score']}/10 |
| **Current Phase** | {project.current_phase.value} |
| **Features** | {feature_analysis['completed']}/{feature_analysis['total']} completed ({feature_analysis['percentage']:.0f}%) |
| **Artifact Quality** | {artifact_analysis['average_score']:.0%} |
| **Project Type** | {project.project_type.value if project.project_type else 'N/A'} |
| **Scale** | {project.scale.value} |

---

## 1. Phase Progression

{phase_analysis}

## 2. Feature Analysis

{feature_analysis['report']}

## 3. Artifact Quality

{artifact_analysis['report']}

## 4. Confidence Assessment

**Score: {confidence['score']}/10** — {confidence['label']}

{confidence['reasoning']}

### Confidence Breakdown
| Factor | Score | Weight | Contribution |
|--------|-------|--------|-------------|
| Phase progression | {confidence['factors']['phase']:.1f}/10 | 25% | {confidence['factors']['phase'] * 0.25:.1f} |
| Feature completion | {confidence['factors']['features']:.1f}/10 | 30% | {confidence['factors']['features'] * 0.30:.1f} |
| Artifact quality | {confidence['factors']['artifacts']:.1f}/10 | 25% | {confidence['factors']['artifacts'] * 0.25:.1f} |
| Validation scores | {confidence['factors']['validation']:.1f}/10 | 20% | {confidence['factors']['validation'] * 0.20:.1f} |

## 5. Recommendations

{recommendations}

---

*Generated by AI Project Playbook System Review*
"""

    return report


def calculate_execution_confidence(project: ProjectState) -> dict:
    """
    Calculate a confidence score (1-10) for the project execution.

    Factors:
    - Phase progression (25%)
    - Feature completion (30%)
    - Artifact quality (25%)
    - Validation scores (20%)
    """
    # Phase progression score (0-10)
    phase_order = [
        Phase.DISCOVERY, Phase.PLANNING, Phase.ROADMAP,
        Phase.IMPLEMENTATION, Phase.DEPLOYMENT, Phase.COMPLETED,
    ]
    try:
        phase_idx = phase_order.index(project.current_phase)
        phase_score = (phase_idx / (len(phase_order) - 1)) * 10
    except ValueError:
        phase_score = 0.0

    # Feature completion score (0-10)
    if project.features:
        completed = sum(1 for f in project.features if f.status == "completed")
        features_score = (completed / len(project.features)) * 10
    else:
        features_score = 5.0  # Neutral if no features yet

    # Artifact quality score (0-10)
    artifact_scores = []
    if "claude_md_score" in project.validation_results:
        artifact_scores.append(project.validation_results["claude_md_score"])
    if "prd_score" in project.validation_results:
        artifact_scores.append(project.validation_results["prd_score"])
    # Include plan scores
    for key, val in project.validation_results.items():
        if key.startswith("plan_") and key.endswith("_score"):
            artifact_scores.append(val)

    artifacts_score = (sum(artifact_scores) / len(artifact_scores) * 10) if artifact_scores else 5.0

    # Validation score — based on whether artifacts passed
    validation_items = []
    if "claude_md_passed" in project.validation_results:
        validation_items.append(1.0 if project.validation_results["claude_md_passed"] else 0.0)
    if "prd_passed" in project.validation_results:
        validation_items.append(1.0 if project.validation_results["prd_passed"] else 0.0)

    validation_score = (sum(validation_items) / len(validation_items) * 10) if validation_items else 5.0

    # Weighted total
    total = (
        phase_score * 0.25
        + features_score * 0.30
        + artifacts_score * 0.25
        + validation_score * 0.20
    )

    score = round(min(max(total, 1.0), 10.0), 1)

    # Label
    if score >= 8:
        label = "High confidence — project is on track"
    elif score >= 6:
        label = "Moderate confidence — some areas need attention"
    elif score >= 4:
        label = "Low confidence — significant issues to address"
    else:
        label = "Critical — project needs major course correction"

    # Reasoning
    reasoning_parts = []
    if phase_score >= 6:
        reasoning_parts.append(f"Good phase progression (currently in {project.current_phase.value})")
    else:
        reasoning_parts.append(f"Early stage — only in {project.current_phase.value} phase")

    if features_score >= 7:
        reasoning_parts.append("Strong feature completion rate")
    elif project.features:
        completed = sum(1 for f in project.features if f.status == "completed")
        reasoning_parts.append(f"Feature completion needs work ({completed}/{len(project.features)})")

    if artifacts_score >= 7:
        reasoning_parts.append("Artifacts meet quality standards")
    elif artifact_scores:
        reasoning_parts.append("Some artifacts could be improved")

    reasoning = "\n".join(f"- {r}" for r in reasoning_parts)

    return {
        "score": score,
        "label": label,
        "reasoning": reasoning,
        "factors": {
            "phase": phase_score,
            "features": features_score,
            "artifacts": artifacts_score,
            "validation": validation_score,
        },
    }


def _analyze_phase_progression(project: ProjectState) -> str:
    """Analyze which phases have been completed."""
    phase_order = [
        ("Discovery", Phase.DISCOVERY),
        ("Planning", Phase.PLANNING),
        ("Roadmap", Phase.ROADMAP),
        ("Implementation", Phase.IMPLEMENTATION),
        ("Deployment", Phase.DEPLOYMENT),
        ("Completed", Phase.COMPLETED),
    ]

    current_found = False
    lines = []
    for name, phase in phase_order:
        if phase == project.current_phase:
            lines.append(f"- [>] **{name}** ← Current")
            current_found = True
        elif not current_found:
            lines.append(f"- [x] {name}")
        else:
            lines.append(f"- [ ] {name}")

    return "\n".join(lines)


def _analyze_features(project: ProjectState) -> dict:
    """Analyze feature completion."""
    total = len(project.features)
    if total == 0:
        return {
            "total": 0,
            "completed": 0,
            "percentage": 0.0,
            "report": "No features defined yet. Complete the Roadmap phase to create features.",
        }

    completed = sum(1 for f in project.features if f.status == "completed")
    in_progress = sum(1 for f in project.features if f.status == "in_progress")
    pending = total - completed - in_progress
    percentage = (completed / total) * 100

    lines = []
    for i, f in enumerate(project.features, 1):
        icon = {"completed": "[x]", "in_progress": "[>]", "pending": "[ ]"}.get(f.status, "[ ]")
        score_key = f"plan_{f.name}_score"
        score = project.validation_results.get(score_key)
        score_str = f" (quality: {score:.0%})" if score is not None else ""
        lines.append(f"{icon} {i}. **{f.name}**{score_str} — {f.description[:60]}")

    report = f"""| Status | Count |
|--------|-------|
| Completed | {completed} |
| In Progress | {in_progress} |
| Pending | {pending} |
| **Total** | **{total}** |

### Feature List

{chr(10).join(lines)}
"""

    return {
        "total": total,
        "completed": completed,
        "percentage": percentage,
        "report": report,
    }


def _analyze_artifacts(project: ProjectState) -> dict:
    """Analyze quality of generated artifacts."""
    evaluator = ArtifactEvaluator()

    lines = []
    scores = []

    if project.claude_md:
        result = evaluator.evaluate_claude_md(project.claude_md)
        scores.append(result.score)
        status = "PASS" if result.passed else "NEEDS WORK"
        lines.append(f"- **CLAUDE.md**: {result.score:.0%} [{status}] — {len(project.claude_md)} chars")
        if result.suggestions:
            for s in result.suggestions[:2]:
                lines.append(f"  - {s}")
    else:
        lines.append("- **CLAUDE.md**: Not generated")

    if project.prd:
        result = evaluator.evaluate_prd(project.prd)
        scores.append(result.score)
        status = "PASS" if result.passed else "NEEDS WORK"
        lines.append(f"- **PRD**: {result.score:.0%} [{status}] — {len(project.prd)} chars")
        if result.suggestions:
            for s in result.suggestions[:2]:
                lines.append(f"  - {s}")
    else:
        lines.append("- **PRD**: Not generated")

    # Count plan scores
    plan_scores = [v for k, v in project.validation_results.items() if k.startswith("plan_") and k.endswith("_score")]
    if plan_scores:
        avg_plan = sum(plan_scores) / len(plan_scores)
        scores.extend(plan_scores)
        lines.append(f"- **Feature Plans**: {avg_plan:.0%} average across {len(plan_scores)} plans")

    avg_score = sum(scores) / len(scores) if scores else 0.0

    return {
        "average_score": avg_score,
        "report": "\n".join(lines),
    }


def _generate_recommendations(project: ProjectState, confidence: dict) -> str:
    """Generate improvement recommendations based on analysis."""
    recommendations = []

    # Phase-based recommendations
    if project.current_phase in (Phase.DISCOVERY, Phase.PLANNING):
        recommendations.append(
            "**Complete discovery and planning** — Rushing to implementation without "
            "proper planning is the #1 cause of project failure."
        )

    # Artifact quality recommendations
    claude_score = project.validation_results.get("claude_md_score", 0)
    if claude_score < 0.7:
        recommendations.append(
            "**Improve CLAUDE.md quality** — Add more specific code patterns, "
            "architecture details, and testing guidelines."
        )

    prd_score = project.validation_results.get("prd_score", 0)
    if prd_score < 0.7:
        recommendations.append(
            "**Enhance PRD** — Add clearer success criteria, feature prioritization (P0/P1), "
            "and known gotchas."
        )

    # Feature-based recommendations
    if project.features:
        completed = sum(1 for f in project.features if f.status == "completed")
        if completed == 0 and project.current_phase == Phase.IMPLEMENTATION:
            recommendations.append(
                "**Start implementing features** — Begin with the first feature and use the PIV Loop. "
                "Don't try to implement everything at once."
            )

    # Confidence-based
    if confidence["score"] < 5:
        recommendations.append(
            "**Review and reset** — Low confidence suggests fundamental issues. "
            "Consider revisiting the PRD or adjusting scope."
        )

    # System-level recommendations
    recommendations.append(
        "**Run validation after each feature** — Use the validation pyramid "
        "(syntax → types → tests → integration → review) before moving to the next feature."
    )

    if not recommendations:
        recommendations.append("Project is on track. Continue with current approach.")

    return "\n".join(f"{i}. {r}" for i, r in enumerate(recommendations, 1))
